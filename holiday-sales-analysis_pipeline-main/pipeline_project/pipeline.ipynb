{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222be6d6-71e3-4357-9f2f-adc9965d24c0",
   "metadata": {},
   "source": [
    "Project Overview: Analyzing Holiday Sales Trends\n",
    "Retail giants like Walmart have increasingly focused on expanding their online presence, with e-commerce playing a crucial role in overall revenue. By the end of 2022, Walmart’s online sales surged to $80 billion, accounting for 13% of its total revenue. A key factor influencing sales patterns is the occurrence of public holidays, such as the Super Bowl, Labor Day, Thanksgiving, and Christmas.\n",
    "\n",
    "For this project, your goal is to build a data pipeline that analyzes supply and demand fluctuations around these holidays while performing an initial data exploration. You will work with two datasets: grocery sales and supplementary market data.\n",
    "\n",
    "Data Sources\n",
    "Grocery Sales Data (grocery_sales.csv)\n",
    "\n",
    "index – Unique identifier for each row\n",
    "Store_ID – Store location identifier\n",
    "Date – Sales week\n",
    "Weekly_Sales – Revenue generated for that store during the specified week\n",
    "Supplementary Data (extra_data.parquet)\n",
    "\n",
    "IsHoliday – Indicates if the sales week includes a public holiday (1 = Yes, 0 = No)\n",
    "Temperature – Recorded temperature on the sales day\n",
    "Fuel_Price – Fuel cost in the store’s region\n",
    "CPI – Consumer Price Index at the time\n",
    "Unemployment – Regional unemployment rate\n",
    "MarkDown1 - MarkDown4 – Promotional markdown amounts\n",
    "Dept – Department number within each store\n",
    "Size – Store size classification\n",
    "Type – Store category based on its size\n",
    "Our task is to clean, integrate, and analyze these datasets to extract meaningful insights into sales trends, particularly around key holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecbc6581-bf0a-4179-9578-d526c90ae2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Extract function is already implemented for you \n",
    "def extract(store_data, extra_data):\n",
    "    store_data = pd.read_csv(store_data)\n",
    "    extra_df = pd.read_csv(extra_data)\n",
    "    merged_df = store_data.merge(extra_df, on = \"index\")\n",
    "    return merged_df\n",
    "\n",
    "# Call the extract() function and store it as the \"merged_df\" variable\n",
    "merged_df = extract(\"grocery_sales.csv\", \"extra_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f18bd40-d092-43fd-aac5-16c2c70b4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transform() function with one parameter: \"raw_data\"\n",
    "import pandas as pd\n",
    "\n",
    "def transform(raw_data):\n",
    "    # Identify columns with missing values\n",
    "    missing_columns = raw_data.columns[raw_data.isnull().any()]\n",
    "    \n",
    "    for column in missing_columns:\n",
    "        # Check if the column is of numerical type\n",
    "        if raw_data[column].dtype in ['float64', 'int64']:\n",
    "            # Fill numerical columns with the mean for most cases\n",
    "            mean_value = raw_data[column].mean()\n",
    "            raw_data[column].fillna(mean_value, inplace=True)\n",
    "        \n",
    "        # Check if the column is categorical (object or category types)\n",
    "        elif raw_data[column].dtype in ['object', 'category']:\n",
    "            # Fill categorical columns with the mode (most frequent value)\n",
    "            mode_value = raw_data[column].mode()[0]\n",
    "            raw_data[column].fillna(mode_value, inplace=True)\n",
    "        \n",
    "        # Check if the column is of datetime type\n",
    "        elif pd.api.types.is_datetime64_any_dtype(raw_data[column]):\n",
    "            # Fill datetime columns using forward fill\n",
    "            raw_data[column].fillna(method=\"ffill\", inplace=True)\n",
    "    \n",
    "    # Remove rows where Weekly_Sales is not greater than 10,000\n",
    "    raw_data = raw_data[raw_data[\"Weekly_Sales\"] > 10000]\n",
    "    \n",
    "    # Add a \"Month\" column extracted from the \"Date\" column\n",
    "    raw_data[\"Month\"] = pd.to_datetime(raw_data[\"Date\"]).dt.month\n",
    "    \n",
    "    # Drop unnecessary columns (you can adjust this as per your specific needs)\n",
    "    columns_to_drop = ['level_0_x', 'level_0_y', 'index', 'Type', 'Size','Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','Date']\n",
    "    raw_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "   # raw_data.rename(columns={\"Weekly_Sales\": \"Avg_Sales\"}, inplace=True)\n",
    "    # Return the cleaned DataFrame\n",
    "    clean_data = raw_data\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ff78f8-87a0-48cf-b230-cf4a64c0e554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nosha\\AppData\\Local\\Temp\\ipykernel_9680\\2161696618.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  raw_data[\"Month\"] = pd.to_datetime(raw_data[\"Date\"]).dt.month\n",
      "C:\\Users\\nosha\\AppData\\Local\\Temp\\ipykernel_9680\\2161696618.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  raw_data.drop(columns=columns_to_drop, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Call the transform() function and pass the merged DataFrame\n",
    "clean_data = transform(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823604c0-25b1-4dd7-93d0-c43b9cd14e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\n",
    "def avg_weekly_sales_per_month(clean_data):\n",
    "    agg_data=clean_data.groupby([\"Month\"])[\"Weekly_Sales\"].agg(\"mean\").reset_index().round(2)\n",
    "    return agg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4388d6c3-e02d-446e-be74-308c771c0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data=avg_weekly_sales_per_month(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04f54e6-a1b5-4a97-bab7-848d2b531ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\n",
    "def load(cleaned_data, agg_data, cleaned_path=\"clean_data.csv\", agg_path=\"agg_data.csv\"):\n",
    "    # Save the cleaned data to a CSV file without the index\n",
    "    cleaned_data.to_csv(cleaned_path, index=False)\n",
    "    \n",
    "    # Save the aggregated data to a CSV file without the index\n",
    "    agg_data.to_csv(agg_path, index=False)\n",
    "    return cleaned_path,agg_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "133e3348-96ac-4a60-b2dd-19b74d89b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_path,agg_path=load(clean_data, agg_data, cleaned_path=\"clean_data.csv\", agg_path=\"agg_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "608a9b9c-6351-4c15-86b7-0c3596725d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\n",
    "def validation(cleaned_path, agg_path):\n",
    "    # Check if the cleaned data CSV file exists\n",
    "    if os.path.exists(cleaned_path):\n",
    "        print(f\"{cleaned_path} exists.\")\n",
    "    else:\n",
    "        print(f\"{cleaned_path} does not exist.\")\n",
    "    \n",
    "    # Check if the aggregated data CSV file exists\n",
    "    if os.path.exists(agg_path):\n",
    "        print(f\"{agg_path} exists.\")\n",
    "    else:\n",
    "        print(f\"{agg_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ec01a69-5694-46c8-bb3b-9adbc1f932c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_data.csv exists.\n",
      "agg_data.csv exists.\n"
     ]
    }
   ],
   "source": [
    "# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\n",
    "validation(cleaned_path=\"clean_data.csv\", agg_path=\"agg_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c45e30-da5e-4ebd-a804-f972baecb978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
